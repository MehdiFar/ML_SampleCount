{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fe73fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import linalg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib import colors\n",
    "import pickle\n",
    "from random import sample\n",
    "import matplotlib.lines as mlines\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Colormap\n",
    "cmap = colors.LinearSegmentedColormap(\n",
    "    'red_blue_classes',\n",
    "    {'red': [(0, 1, 1), (1, 0.7, 0.7)],\n",
    "     'green': [(0, 0.7, 0.7), (1, 0.7, 0.7)],\n",
    "     'blue': [(0, 0.7, 0.7), (1, 1, 1)]})\n",
    "plt.cm.register_cmap(cmap=cmap)\n",
    "\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Generate datasets\n",
    "def dataset_fixed_cov(n,dim):\n",
    "    '''Generate 2 Gaussians samples with the same covariance matrix'''\n",
    "    np.random.seed(0)\n",
    "    C = np.array([[0., -0.23], [0.83, 0.23]]) \n",
    "    X = np.r_[np.dot(3*np.random.randn(n, dim), C) + np.array([-1.0, -1.0]),\n",
    "              np.dot(3*np.random.randn(n, dim), C) + np.array([1.0, 1.0])]\n",
    "    y = np.hstack((np.zeros(n), np.ones(n)))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def dataset_cov(n,dim):\n",
    "    '''Generate 2 Gaussians samples with different covariance matrices'''\n",
    "    np.random.seed(0)\n",
    "\n",
    "    C = np.array([[0.0, -1.0], [2.5, 1.7]]) * 1.   #[[0., -1.], [2.5, 1.7]]) * 2.    + np.array([0,0])] : 0.9\n",
    "    CT = np.array([[0.0, 2.5], [-1.0, 1.7]])\n",
    "\n",
    "    X = np.r_[np.dot(np.random.randn(n, dim), C),\n",
    "              np.dot(np.random.randn(n, dim), CT) + np.array([0, 0])]\n",
    "    y = np.hstack((np.zeros(n), np.ones(n)))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def plot_raw_data(X, y, X_test, y_test):\n",
    "    # scatter plot the entire training and testing cases\n",
    "    splot = plt.subplot(1, 2, 1)\n",
    "\n",
    "    fig = matplotlib.pyplot.gcf()\n",
    "    fig.set_size_inches(10, 4)\n",
    "\n",
    "    # splot = plt.subplot(2, 2,figsize=(15,15))\n",
    "    # splot = plt.subplots(figsize=(3, 4))\n",
    "\n",
    "\n",
    "    plt.title('Training, %i samples' %X.shape[0])\n",
    "    plt.ylabel('Data with\\n different covariance')\n",
    "    X_neg = X[y == 0]\n",
    "    X_pos = X[y == 1]\n",
    "\n",
    "    plt.scatter(X_neg[:, 0], X_neg[:, 1], marker='.', color='red')\n",
    "    plt.scatter(X_pos[:, 0], X_pos[:, 1], marker='.', color='blue')\n",
    "\n",
    "    nx, ny = 900, 300\n",
    "    plt.xlim(-10, 10)\n",
    "    plt.ylim(-10, 10)\n",
    "    x_min, x_max = plt.xlim()\n",
    "    y_min, y_max = plt.ylim()\n",
    "        \n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, nx),\n",
    "                     np.linspace(y_min, y_max, ny))\n",
    "\n",
    "    TPs = mlines.Line2D([], [], color='blue', marker='.', linestyle='None',\n",
    "                      markersize=5, label='Positives')\n",
    "    TNs = mlines.Line2D([], [], color='red', marker='.', linestyle='None',\n",
    "                      markersize=5, label='Negatives')\n",
    "        \n",
    "    plt.legend(handles=[TPs, TNs],loc='lower right')\n",
    "\n",
    "    splot = plt.subplot(1, 2, 2)\n",
    "        \n",
    "    plt.title('Testing, %i samples' %X_test.shape[0])\n",
    "    plt.ylabel('Data with\\n different covariance')\n",
    "    X_neg = X_test[y_test == 0]\n",
    "    X_pos = X_test[y_test == 1]\n",
    "\n",
    "    plt.scatter(X_neg[:, 0], X_neg[:, 1], marker='.', color='red')\n",
    "    plt.scatter(X_pos[:, 0], X_pos[:, 1], marker='.', color='blue')\n",
    "\n",
    "    nx, ny = 900, 300\n",
    "    plt.xlim(-10, 10)\n",
    "    plt.ylim(-10, 10)\n",
    "    x_min, x_max = plt.xlim()\n",
    "    y_min, y_max = plt.ylim()\n",
    "        \n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, nx),\n",
    "                     np.linspace(y_min, y_max, ny))\n",
    "\n",
    "    TPs = mlines.Line2D([], [], color='blue', marker='.', linestyle='None',\n",
    "                          markersize=5, label='Positives')\n",
    "    TNs = mlines.Line2D([], [], color='red', marker='.', linestyle='None',\n",
    "                          markersize=5, label='Negatives')\n",
    "        \n",
    "    plt.legend(handles=[TPs, TNs],loc='lower right')\n",
    "   \n",
    "    return plt\n",
    "\n",
    "\n",
    "def plot_hyperplane_data(lda, X, y, X_test, y_pred, y_truth):\n",
    "    # scatter plot train and test images with class separing hyperplane\n",
    "    splot = plt.subplot(1, 2, 1)\n",
    "    fig = matplotlib.pyplot.gcf()\n",
    "    fig.set_size_inches(10, 4)\n",
    "\n",
    "    plt.title('Training, %i samples' %X.shape[0])\n",
    "    plt.ylabel('Data with\\n fixed covariance')\n",
    "    X_neg = X[y == 0]\n",
    "    X_pos = X[y == 1]\n",
    "\n",
    "    plt.scatter(X_neg[:, 0], X_neg[:, 1], marker='.', color='red')\n",
    "    plt.scatter(X_pos[:, 0], X_pos[:, 1], marker='.', color='blue')\n",
    "\n",
    "    nx, ny = 600, 300\n",
    "    plt.xlim(-10, 10)\n",
    "    plt.ylim(-10, 10)\n",
    "    x_min, x_max = plt.xlim()\n",
    "    y_min, y_max = plt.ylim()\n",
    "        \n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, nx),\n",
    "                         np.linspace(y_min, y_max, ny))\n",
    "    Z = lda.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z[:, 1].reshape(xx.shape)\n",
    "    plt.pcolormesh(xx, yy, Z, cmap='red_blue_classes',\n",
    "                   norm=colors.Normalize(0., 1.), zorder=0, shading='auto')\n",
    "    plt.contour(xx, yy, Z, [0.5], linewidths=2., colors='white')\n",
    "\n",
    "\n",
    "    xx_train = xx\n",
    "    yy_train = yy\n",
    "    Z_train = Z\n",
    "\n",
    "    splot = plt.subplot(1, 2, 2)\n",
    "        \n",
    "    plt.ylabel('Data with\\n fixed covariance')\n",
    "\n",
    "    tp = (y_truth == y_pred)  # True Positive\n",
    "    tp0, tp1 = tp[y_truth == 0], tp[y_truth == 1]\n",
    "    X0, X1 = X_test[y_truth == 0], X_test[y_truth == 1]\n",
    "    X0_tp, X0_fp = X0[tp0], X0[~tp0]\n",
    "    X1_tp, X1_fp = X1[tp1], X1[~tp1]\n",
    "\n",
    "    plt.scatter(X0_tp[:, 0], X0_tp[:, 1], marker='.', color='red')\n",
    "    plt.scatter(X0_fp[:, 0], X0_fp[:, 1], marker='x',\n",
    "            s=20, color='black')  # dark red\n",
    "\n",
    "    # class 1: dots\n",
    "    plt.scatter(X1_tp[:, 0], X1_tp[:, 1], marker='.', color='blue')\n",
    "    plt.scatter(X1_fp[:, 0], X1_fp[:, 1], marker='x',\n",
    "            s=20, color='black')  # dark blue\n",
    "\n",
    "    # class 0 and 1 : areas\n",
    "    nx, ny = 200, 100\n",
    "    plt.xlim(-10, 10)\n",
    "    plt.ylim(-10, 10)\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, nx),\n",
    "                     np.linspace(y_min, y_max, ny))\n",
    "    Z = lda.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z[:, 1].reshape(xx.shape)\n",
    "    plt.pcolormesh(xx_train, yy_train, Z_train, cmap='red_blue_classes',\n",
    "               norm=colors.Normalize(0., 1.), zorder=0, shading='auto')\n",
    "    plt.contour(xx_train, yy_train, Z_train, [0.5], linewidths=2., colors='white')\n",
    "\n",
    "    plt.title('Testing, ' \"{0:.2%} Accuracy\".format(sum(tp)/len(tp)))\n",
    "\n",
    "    TPs = mlines.Line2D([], [], color='blue', marker='.', linestyle='None',\n",
    "                      markersize=5, label='True Positive')\n",
    "    TNs = mlines.Line2D([], [], color='red', marker='.', linestyle='None',\n",
    "                          markersize=5, label='True Negatives')\n",
    "    misclassified = mlines.Line2D([], [], color='black', marker='x', linestyle='None',\n",
    "                      markersize=5, label='missclassified')\n",
    "    plt.legend(handles=[TPs, TNs, misclassified],loc='lower right')\n",
    "\n",
    "    \n",
    "    return splot, sum(tp)/len(tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6198ace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "### Linear Discriminant Analysis ###\n",
    "####################################\n",
    "\n",
    "\n",
    "train_size = 200        # number of positive and negative training samples should be > 3\n",
    "test_size = 50          # number of positive and negative testing samples should be > 3\n",
    "varData_train = dataset_fixed_cov(train_size,2)\n",
    "varData_test = dataset_fixed_cov(test_size,2)\n",
    "splot = plot_raw_data(varData_train[0], varData_train[1], varData_test[0], varData_test[1])\n",
    "splot.tight_layout()\n",
    "splot.subplots_adjust(top=0.9)\n",
    "splot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf57ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.1     # ratio of  data to use for training the classifier. should be in (0,1]\n",
    "all_indices = list(range(varData_train[0].shape[0]))\n",
    "all_indices_positive = [i for i, x in enumerate(varData_train[1]) if x]\n",
    "all_indices_negative = [i for i, x in enumerate(varData_train[1]) if not x]\n",
    "train_size = int(len(all_indices_positive)*train_ratio)\n",
    "if(train_size < 2):\n",
    "    train_size = 2\n",
    "\n",
    "\n",
    "train_index_positive = sample(all_indices_positive,train_size)\n",
    "train_index_negative = sample(all_indices_negative,train_size)\n",
    "train_index = train_index_negative + train_index_positive\n",
    "\n",
    "X_train = varData_train[0][train_index]\n",
    "y_train = varData_train[1][train_index]\n",
    "X_test = varData_test[0]\n",
    "y_test = varData_test[1]\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(solver=\"svd\", store_covariance=True)\n",
    "y_pred = lda.fit(X_train, y_train).predict(X_test)\n",
    "splot,accuracy = plot_hyperplane_data(lda, X_train, y_train, X_test, y_pred, y_test)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e0a3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "### Quadratic Discriminant Analysis ###\n",
    "#######################################\n",
    "\n",
    "\n",
    "train_size = 150        # number of positive and negative training samples should be > 3\n",
    "test_size = 50          # number of positive and negative testing samples should be > 3\n",
    "\n",
    "varData_train = dataset_cov(train_size,2)\n",
    "varData_test = dataset_cov(test_size,2)\n",
    "splot = plot_raw_data(varData_train[0], varData_train[1], varData_test[0], varData_test[1])\n",
    "splot.tight_layout()\n",
    "splot.subplots_adjust(top=0.9)\n",
    "splot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404aa003",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.1     # ratio of  data to use for training the classifier. should be in (0,1]\n",
    "all_indices = list(range(varData_train[0].shape[0]))\n",
    "all_indices_positive = [i for i, x in enumerate(varData_train[1]) if x]\n",
    "all_indices_negative = [i for i, x in enumerate(varData_train[1]) if not x]\n",
    "train_size = int(len(all_indices_positive)*train_ratio)\n",
    "if(train_size < 3):\n",
    "    train_size = 3\n",
    "\n",
    "\n",
    "train_index_positive = sample(all_indices_positive,train_size)\n",
    "train_index_negative = sample(all_indices_negative,train_size)\n",
    "train_index = train_index_negative + train_index_positive\n",
    "\n",
    "\n",
    "X_train = varData_train[0][train_index]\n",
    "y_train = varData_train[1][train_index]\n",
    "X_test = varData_test[0]\n",
    "y_test = varData_test[1]\n",
    "\n",
    "qda = QuadraticDiscriminantAnalysis(store_covariance=True)\n",
    "y_pred = qda.fit(X_train, y_train).predict(X_test)\n",
    "splot,accuracy = plot_hyperplane_data(qda, X_train, y_train, X_test, y_pred, y_test)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f306d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
